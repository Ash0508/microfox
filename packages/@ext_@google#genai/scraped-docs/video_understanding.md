{
  "url": "https://ai.google.dev/gemini-api/docs/video-understanding",
  "content": "Skip to main content\nModels\nMore\nGemini API docs\nAPI Reference\nCookbook\nCommunity\nSolutions\nMore\nCode assistance\nMore\nShowcase\nMore\nCommunity\nMore\nGet started\nOverview\nQuickstart\nAPI keys\nLibraries\nOpenAI compatibility\nModels\nAll models\nPricing\nRate limits\nBilling info\nModel Capabilities\nText generation\nImage generation\nVideo generation\nLong context\nStructured output\nThinking\nFunction calling\nDocument understanding\nImage understanding\nVideo understanding\nAudio understanding\nCode execution\nGrounding with Google Search\nGuides\nPrompt engineering\nLive API\nContext caching\nFiles API\nToken counting\nFine-tuning\nEmbeddings\nSafety\nResources\nMigrate to Gen AI SDK\nRelease notes\nAPI troubleshooting\nAI Studio\nCloud\nPolicies\nTerms of service\nAvailable regions\nAdditional usage polices\nIntroducing Gemini 2.5 Flash, Veo 2, and updates to the Live API Learn more\nHome\nGemini API\nModels\nSend feedback\nVideo understanding\nOn this page\nVideo input\nUpload a video file\nPass video data inline\nInclude a YouTube URL\nRefer to timestamps in the content\nTranscribe video and provide visual descriptions\nSupported video formats\nTechnical details about videos\n\nGemini models can process videos, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:\n\nDescribe, segment, and extract information from videos up to 90 minutes long\nAnswer questions about video content\nRefer to specific timestamps within a video\n\nGemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible. This guide shows how to use the Gemini API to generate text responses based on video inputs.\n\nBefore you begin\n\nBefore calling the Gemini API, ensure you have your SDK of choice installed, and a Gemini API key configured and ready to use.\n\nVideo input\n\nYou can provide videos as input to Gemini in the following ways:\n\nUpload a video file using the File API before making a request to generateContent. Use this method for files larger than 20MB, videos longer than approximately 1 minute, or when you want to reuse the file across multiple requests.\nPass inline video data with the request to generateContent. Use this method for smaller files (<20MB) and shorter durations.\nInclude a YouTube URL directly in the prompt.\nUpload a video file\n\nYou can use the Files API to upload a video file. Always use the Files API when the total request size (including the file, text prompt, system instructions, etc.) is larger than 20 MB, the video duration is significant, or if you intend to use the same video in multiple prompts.\n\nThe File API accepts video file formats directly. This example uses the short NASA film \"Jupiter's Great Red Spot Shrinks and Grows\". Credit: Goddard Space Flight Center (GSFC)/David Ladd (2018).\n\n\"Jupiter's Great Red Spot Shrinks and Grows\" is in the public domain and does not show identifiable people. (NASA image and media usage guidelines.)\n\nThe following code downloads the sample video, uploads it using the File API, waits for it to be processed, and then uses the file reference in a generateContent request.\n\nPython\nJavaScript\nGo\nREST\nfrom google import genai\n\nclient = genai.Client(api_key=\"GOOGLE_API_KEY\")\n\nmyfile = client.files.upload(file=\"path/to/sample.mp4\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", contents=[myfile, \"Summarize this video. Then create a quiz with an answer key based on the information in this video.\"]\n)\n\nprint(response.text)\n\n\nTo learn more about working with media files, see Files API.\n\nPass video data inline\n\nInstead of uploading a video file using the File API, you can pass smaller videos directly in the request to generateContent. This is suitable for shorter videos under 20MB total request size.\n\nHere's an example of providing inline video data:\n\nPython\nJavaScript\nREST\n# Only for videos of size <20Mb\nvideo_file_name = \"/path/to/your/video.mp4\"\nvideo_bytes = open(video_file_name, 'rb').read()\n\nresponse = client.models.generate_content(\n    model='models/gemini-2.0-flash',\n    contents=types.Content(\n        parts=[\n            types.Part(\n                inline_data=types.Blob(data=video_bytes, mime_type='video/mp4')\n            ),\n            types.Part(text='Please summarize the video in 3 sentences.')\n        ]\n    )\n)\n\nInclude a YouTube URL\nPreview: The YouTube URL feature is in preview and is available at no charge. Pricing and rate limits are likely to change.\n\nThe Gemini API and AI Studio support YouTube URLs as a file data Part. You can include a YouTube URL with a prompt asking the model to summarize, translate, or otherwise interact with the video content.\n\nLimitations:\n\nYou can't upload more than 8 hours of YouTube video per day.\nYou can upload only 1 video per request.\nYou can only upload public videos (not private or unlisted videos).\n\nThe following example shows how to include a YouTube URL with a prompt:\n\nPython\nJavaScript\nGo\nREST\nresponse = client.models.generate_content(\n    model='models/gemini-2.0-flash',\n    contents=types.Content(\n        parts=[\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=9hE5-98ZeCg')\n            ),\n            types.Part(text='Please summarize the video in 3 sentences.')\n        ]\n    )\n)\n\nRefer to timestamps in the content\n\nYou can ask questions about specific points in time within the video using timestamps of the form MM:SS.\n\nPython\nJavaScript\nGo\nREST\nprompt = \"What are the examples given at 00:05 and 00:10 supposed to show us?\" # Adjusted timestamps for the NASA video\n\nTranscribe video and provide visual descriptions\n\nThe Gemini models can transcribe and provide visual descriptions of video content by processing both the audio track and visual frames. For visual descriptions, the model samples the video at a rate of 1 frame per second. This sampling rate may affect the level of detail in the descriptions, particularly for videos with rapidly changing visuals.\n\nPython\nJavaScript\nGo\nREST\nprompt = \"Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.\"\n\nSupported video formats\n\nGemini supports the following video format MIME types:\n\nvideo/mp4\nvideo/mpeg\nvideo/mov\nvideo/avi\nvideo/x-flv\nvideo/mpg\nvideo/webm\nvideo/wmv\nvideo/3gpp\nTechnical details about videos\nSupported models & context: All Gemini 2.0 and 2.5 models can process video data.\nModels with a 2M context window can process videos up to 2 hours long at default media resolution or 6 hours long at low media resolution, while models with a 1M context window can process videos up to 1 hour long at default media resolution or 3 hours long at low media resolution.\nFile API processing: When using the File API, videos are sampled at 1 frame per second (FPS) and audio is processed at 1Kbps (single channel). Timestamps are added every second.\nThese rates are subject to change in the future for improvements in inference.\nToken calculation: Each second of video is tokenized as follows:\nIndividual frames (sampled at 1 FPS):\nIf mediaResolution is set to low, frames are tokenized at 66 tokens per frame.\nOtherwise, frames are tokenized at 258 tokens per frame.\nAudio: 32 tokens per second.\nMetadata is also included.\nTotal: Approximately 300 tokens per second of video at default media resolution, or 100 tokens per second of video at low media resolution.\nTimestamp format: When referring to specific moments in a video within your prompt, use the MM:SS format (e.g., 01:15 for 1 minute and 15 seconds).\nBest practices:\nUse only one video per prompt request for optimal results.\nIf combining text and a single video, place the text prompt after the video part in the contents array.\nBe aware that fast action sequences might lose detail due to the 1 FPS sampling rate. Consider slowing down such clips if necessary.\nWhat's next\n\nThis guide shows how to upload video files and generate text outputs from video inputs. To learn more, see the following resources:\n\nSystem instructions: System instructions let you steer the behavior of the model based on your specific needs and use cases.\nFiles API: Learn more about uploading and managing files for use with Gemini.\nFile prompting strategies: The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\nSafety guidance: Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-05-12 UTC.\n\nTerms\nPrivacy",
  "updatedAt": "2025-05-13T03:15:50.178Z"
}