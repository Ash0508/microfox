{
  "url": "https://ai.google.dev/gemini-api/docs/caching",
  "content": "Skip to main content\nSign in\nModels\nMore\nGemini API docs\nAPI Reference\nCookbook\nCommunity\nSolutions\nMore\nCode assistance\nMore\nShowcase\nMore\nCommunity\nMore\nGet started\nOverview\nQuickstart\nAPI keys\nLibraries\nOpenAI compatibility\nModels\nAll models\nPricing\nRate limits\nBilling info\nModel Capabilities\nText generation\nImage generation\nVideo generation\nLong context\nStructured output\nThinking\nFunction calling\nDocument understanding\nImage understanding\nVideo understanding\nAudio understanding\nCode execution\nGrounding with Google Search\nGuides\nPrompt engineering\nLive API\nContext caching\nFiles API\nToken counting\nFine-tuning\nEmbeddings\nSafety\nResources\nMigrate to Gen AI SDK\nRelease notes\nAPI troubleshooting\nAI Studio\nCloud\nPolicies\nTerms of service\nAvailable regions\nAdditional usage polices\nIntroducing Gemini 2.5 Flash, Veo 2, and updates to the Live API Learn more\nHome\nGemini API\nModels\nSend feedback\nContext caching\nOn this page\nImplicit caching\nExplicit caching\nWhen to use explicit caching\nHow explicit caching reduces costs\nAdditional considerations\n\nPython JavaScript Go REST\n\nIn a typical AI workflow, you might pass the same input tokens over and over to a model. The Gemini API offers two different caching mechanisms:\n\nImplicit caching (automatic, no cost saving guarantee)\nExplicit caching (manual, cost saving guarantee)\n\nImplicit caching is enabled on Gemini 2.5 models by default. If a request contains content that is a cache hit, we automatically pass the cost savings back to you.\n\nExplicit caching is useful in cases where you want to guarantee cost savings, but with some added developer work.\n\nImplicit caching\n\nImplicit caching is enabled by default for all Gemini 2.5 models. We automatically pass on cost savings if your request hits caches. There is nothing you need to do in order to enable this. It is effective as of May 8th, 2025. The minimum input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro.\n\nTo increase the chance of an implicit cache hit:\n\nTry putting large and common contents at the beginning of your prompt\nTry to send requests with similar prefix in a short amount of time\n\nYou can see the number of tokens which were cache hits in the response object's usage_metadata field.\n\nExplicit caching\n\nUsing the Gemini API explicit caching feature, you can pass some content to the model once, cache the input tokens, and then refer to the cached tokens for subsequent requests. At certain volumes, using cached tokens is lower cost than passing in the same corpus of tokens repeatedly.\n\nWhen you cache a set of tokens, you can choose how long you want the cache to exist before the tokens are automatically deleted. This caching duration is called the time to live (TTL). If not set, the TTL defaults to 1 hour. The cost for caching depends on the input token size and how long you want the tokens to persist.\n\nThis section assumes that you've installed a Gemini SDK (or have curl installed) and that you've configured an API key, as shown in the quickstart.\n\nWhen to use explicit caching\n\nContext caching is particularly well suited to scenarios where a substantial initial context is referenced repeatedly by shorter requests. Consider using context caching for use cases such as:\n\nChatbots with extensive system instructions\nRepetitive analysis of lengthy video files\nRecurring queries against large document sets\nFrequent code repository analysis or bug fixing\nHow explicit caching reduces costs\n\nContext caching is a paid feature designed to reduce overall operational costs. Billing is based on the following factors:\n\nCache token count: The number of input tokens cached, billed at a reduced rate when included in subsequent prompts.\nStorage duration: The amount of time cached tokens are stored (TTL), billed based on the TTL duration of cached token count. There are no minimum or maximum bounds on the TTL.\nOther factors: Other charges apply, such as for non-cached input tokens and output tokens.\n\nFor up-to-date pricing details, refer to the Gemini API pricing page. To learn how to count tokens, see the Token guide.\n\nAdditional considerations\n\nKeep the following considerations in mind when using context caching:\n\nThe minimum input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro. The maximum is the same as the maximum for the given model. (For more on counting tokens, see the Token guide).\nThe model doesn't make any distinction between cached tokens and regular input tokens. Cached content is a prefix to the prompt.\nThere are no special rate or usage limits on context caching; the standard rate limits for GenerateContent apply, and token limits include cached tokens.\nThe number of cached tokens is returned in the usage_metadata from the create, get, and list operations of the cache service, and also in GenerateContent when using the cache.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-05-08 UTC.\n\nTerms\nPrivacy",
  "updatedAt": "2025-05-13T03:18:50.125Z"
}