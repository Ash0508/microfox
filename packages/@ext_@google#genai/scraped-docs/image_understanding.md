{
  "url": "https://ai.google.dev/gemini-api/docs/image-understanding",
  "content": "Skip to main content\nSign in\nModels\nMore\nGemini API docs\nAPI Reference\nCookbook\nCommunity\nSolutions\nMore\nCode assistance\nMore\nShowcase\nMore\nCommunity\nMore\nGet started\nOverview\nQuickstart\nAPI keys\nLibraries\nOpenAI compatibility\nModels\nAll models\nPricing\nRate limits\nBilling info\nModel Capabilities\nText generation\nImage generation\nVideo generation\nLong context\nStructured output\nThinking\nFunction calling\nDocument understanding\nImage understanding\nVideo understanding\nAudio understanding\nCode execution\nGrounding with Google Search\nGuides\nPrompt engineering\nLive API\nContext caching\nFiles API\nToken counting\nFine-tuning\nEmbeddings\nSafety\nResources\nMigrate to Gen AI SDK\nRelease notes\nAPI troubleshooting\nAI Studio\nCloud\nPolicies\nTerms of service\nAvailable regions\nAdditional usage polices\nIntroducing Gemini 2.5 Flash, Veo 2, and updates to the Live API Learn more\nHome\nGemini API\nModels\nSend feedback\nImage understanding\nOn this page\nImage input\nUpload an image file\nPass image data inline\nPrompting with multiple images\nGet a bounding box for an object\nImage segmentation\nSupported image formats\nTechnical details about images\n\nGemini models can process images, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:\n\nCaption and answer questions about images\nTranscribe and reason over PDFs, including up to 2 million tokens\nDetect objects in an image and return bounding box coordinates for them\nSegment objects within an image\n\nGemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible. This guide shows how to use the Gemini API to generate text responses based on image inputs and perform common image understanding tasks.\n\nBefore you begin\n\nBefore calling the Gemini API, ensure you have your SDK of choice installed, and a Gemini API key configured and ready to use.\n\nImage input\n\nYou can provide images as input to Gemini in the following ways:\n\nUpload an image file using the File API before making a request to generateContent. Use this method for files larger than 20MB or when you want to reuse the file across multiple requests.\nPass inline image data with the request to generateContent. Use this method for smaller files (<20MB total request size) or images fetched directly from URLs.\nUpload an image file\n\nYou can use the Files API to upload an image file. Always use the Files API when the total request size (including the file, text prompt, system instructions, etc.) is larger than 20 MB, or if you intend to use the same image in multiple prompts.\n\nThe following code uploads an image file and then uses the file in a call to generateContent.\n\nPython\nJavaScript\nGo\nREST\nfrom google import genai\n\nclient = genai.Client(api_key=\"GOOGLE_API_KEY\")\n\nmy_file = client.files.upload(file=\"path/to/sample.jpg\")\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[my_file, \"Caption this image.\"],\n)\n\nprint(response.text)\n\n\nTo learn more about working with media files, see Files API.\n\nPass image data inline\n\nInstead of uploading an image file, you can pass inline image data in the request to generateContent. This is suitable for smaller images (less than 20MB total request size) or images fetched directly from URLs.\n\nYou can provide image data as Base64 encoded strings or by reading local files directly (depending on the SDK).\n\nLocal image file:\n\nPython\nJavaScript\nGo\nREST\n  from google.genai import types\n\n  with open('path/to/small-sample.jpg', 'rb') as f:\n      image_bytes = f.read()\n\n  response = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=[\n      types.Part.from_bytes(\n        data=image_bytes,\n        mime_type='image/jpeg',\n      ),\n      'Caption this image.'\n    ]\n  )\n\n  print(response.text)\n\n\nImage from URL:\n\nPython\nJavaScript\nGo\nREST\nfrom google import genai\nfrom google.genai import types\n\nimport requests\n\nimage_path = \"https://goo.gle/instrument-img\"\nimage_bytes = requests.get(image_path).content\nimage = types.Part.from_bytes(\n  data=image_bytes, mime_type=\"image/jpeg\"\n)\n\nclient = genai.Client(api_key=\"GOOGLE_API_KEY\")\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash-exp\",\n    contents=[\"What is this image?\", image],\n)\n\nprint(response.text)\n\n\nA few things to keep in mind about inline image data:\n\nThe maximum total request size is 20 MB, which includes text prompts, system instructions, and all files provided inline. If your file's size will make the total request size exceed 20 MB, then use the Files API to upload an image file for use in the request.\nIf you're using an image sample multiple times, it's more efficient to upload an image file using the File API.\nPrompting with multiple images\n\nYou can provide multiple images in a single prompt by including multiple image Part objects in the contents array. These can be a mix of inline data (local files or URLs) and File API references.\n\nPython\nJavaScript\nGo\nREST\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client(api_key=\"GOOGLE_API_KEY\")\n\n# Upload the first image\nimage1_path = \"path/to/image1.jpg\"\nuploaded_file = client.files.upload(file=image1_path)\n\n# Prepare the second image as inline data\nimage2_path = \"path/to/image2.png\"\nwith open(image2_path, 'rb') as f:\n    img2_bytes = f.read()\n\n# Create the prompt with text and multiple images\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[\n        \"What is different between these two images?\",\n        uploaded_file,  # Use the uploaded file reference\n        types.Part.from_bytes(\n            data=img2_bytes,\n            mime_type='image/png'\n        )\n    ]\n)\n\nprint(response.text)\n\nGet a bounding box for an object\n\nGemini models are trained to identify objects in an image and provide their bounding box coordinates. The coordinates are returned relative to the image dimensions, scaled to [0, 1000]. You need to descale these coordinates based on your original image size.\n\nPython\nJavaScript\nGo\nREST\nprompt = \"Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n\n\nYou can use bounding boxes for object detection and localization within images and video. By accurately identifying and delineating objects with bounding boxes, you can unlock a wide range of applications and enhance the intelligence of your projects.\n\nKey benefits\nSimple: Integrate object detection capabilities into your applications with ease, regardless of your computer vision expertise.\nCustomizable: Produce bounding boxes based on custom instructions (e.g. \"I want to see bounding boxes of all the green objects in this image\"), without having to train a custom model.\nTechnical details\nInput: Your prompt and associated images or video frames.\nOutput: Bounding boxes in the [y_min, x_min, y_max, x_max] format. The top left corner is the origin. The x and y axis go horizontally and vertically, respectively. Coordinate values are normalized to 0-1000 for every image.\nVisualization: AI Studio users will see bounding boxes plotted within the UI.\n\nFor Python developers, try the 2D spatial understanding notebook or the experimental 3D pointing notebook.\n\nNormalize coordinates\n\nThe model returns bounding box coordinates in the format [y_min, x_min, y_max, x_max]. To convert these normalized coordinates to the pixel coordinates of your original image, follow these steps:\n\nDivide each output coordinate by 1000.\nMultiply the x-coordinates by the original image width.\nMultiply the y-coordinates by the original image height.\n\nTo explore more detailed examples of generating bounding box coordinates and visualizing them on images, review the Object Detection cookbook example.\n\nImage segmentation\n\nStarting with the Gemini 2.5 models, Gemini models are trained to not only detect items but also segment them and provide a mask of their contours.\n\nThe model predicts a JSON list, where each item represents a segmentation mask. Each item has a bounding box (\"box_2d\") in the format [y0, x0, y1, x1] with normalized coordinates between 0 and 1000, a label (\"label\") that identifies the object, and finally the segmentation mask inside the bounding box, as base64 encoded png that is a probability map with values between 0 and 255. The mask needs to be resized to match the bounding box dimensions, then binarized at your confidence threshold (127 for the midpoint).\n\nPython\nJavaScript\nGo\nREST\nprompt = \"\"\"\n  Give the segmentation masks for the wooden and glass items.\n  Output a JSON list of segmentation masks where each entry contains the 2D\n  bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and\n  the text label in the key \"label\". Use descriptive labels.\n\"\"\"\n\nMask of the wooden and glass objects found on the picture\n\nCheck the segmentation example in the cookbook guide for a more detailed example.\n\nSupported image formats\n\nGemini supports the following image format MIME types:\n\nPNG - image/png\nJPEG - image/jpeg\nWEBP - image/webp\nHEIC - image/heic\nHEIF - image/heif\nTechnical details about images\nFile limit: Gemini 2.5 Pro, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a maximum of 3,600 image files per request.\nToken calculation:\nGemini 1.5 Flash and Gemini 1.5 Pro: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized to 768x768), with each tile costing 258 tokens.\nGemini 2.0 Flash: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled into 768x768 pixel tiles, each costing 258 tokens.\nBest practices:\nEnsure images are correctly rotated.\nUse clear, non-blurry images.\nWhen using a single image with text, place the text prompt after the image part in the contents array.\nWhat's next\n\nThis guide shows how to upload image files and generate text outputs from image inputs. To learn more, see the following resources:\n\nSystem instructions: System instructions let you steer the behavior of the model based on your specific needs and use cases.\nVideo understanding: Learn how to work with video inputs.\nFiles API: Learn more about uploading and managing files for use with Gemini.\nFile prompting strategies: The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\nSafety guidance: Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\nSend feedback\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-05-12 UTC.\n\nTerms\nPrivacy",
  "updatedAt": "2025-05-13T03:15:20.633Z"
}